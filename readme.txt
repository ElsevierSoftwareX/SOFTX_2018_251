This ART data analyzer package is a python package that analyze the physical quantities change between two atomic configurations of each possible event search of using ART. The goal is to correlate the change in atomic structure with the change of activation energy. 

Currently, it can output the information such as displacement, atomic strain (e.g. the volumetric and shear strain) as a statistical distribution for both each event. It also implement the physically meaningful atomic structure descriptor SOAP matrix for a complete representation of atomic structure within a local environment.

This package has the following modules and sub-packages:

/src contains all the source code of various libraries, majorly in python, it may also contains other code such as Matlab or C code

data_reader: extracting data from various data file formats (default format lammps dump file), read data into a pandas.Dataframe

event_selector: locate the accepted event to be analyzed while preventing redundant event search
Event_redudant_check that implement redundancy check of all possible pairs events passed two stage 1 criteria

Util: utilities containing various classes and functions for other modules

calculator package:
	strain_calculator: calculate the atomic strain for a specific event

	stress_calculator, voronoi analysis coming soon

visualator package:
	strain_visualator: visualize the atomic strain from stored atomic strain results 	in results.pkl file


/examples: contains examples of executable running scripts and demo data. The demo example system is CuZr metallic glass.

/scripts: contains executable scripts that will be directly invoked on the command line

/tests: contains unittests to test various modules



Operating system:
This python package depends on some python packages such as numpy, pandas, matplotlib, python-tk,scipy, mpl_toolkits, scikit-learn, pathos. install easy_install, pip by sudo apt-get install python-pip python-dev build-essential, install previous mentioned python package by python -m pip install --user numpy scipy matplotlib jupyter pandas sympy nose. Mac O.S mpl_toolkits under matplotlib library may not be a package and need user to manually make it a package to import. User need to verify the successful installation of these dependencies in their OS. The package has been tested on python 2.7. Python 3 user may need further tests
A few notes:
if pip install does not work, especially for pip version 9.0.1, user need to upgrade their pip pip-10.0.1 according to here: https://stackoverflow.com/questions/49768770/not-able-to-install-python-packages-ssl-tlsv1-alert-protocol-version.  Then it can install package such as scikit-learn by pip install —-user scikit-learn


User also need to install pathos for a unified event level parallel computation by using the uitl.operation_on_events function by pip install --user pathos. Pathos can serialize any function that are not defined at the top level of the module that greatly favor the implementation. Check: https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error. It seems that any file that call util.operations_on_events() will need to import os, pickle numpy as np.

For user who actively use other python and packages version and need to create an isolated python package environment just for using this ART_data analyzer package without interfering with your global python environment, please use virtualenv path/to/ART_data_analyzer to create a new virtual python environmental for the path/to/ART_data_analyzer (if need inheriting python from global environment python, use --system-site-packages flag like virtualenv --system-site-packages path/to/ART_data_analyzer), then it will install /bin /include /lib under path/to/ART_data_analyzer to include python packages for this virtual environment. The bin/activate is bash script used to activate this virtual python environment by source /ART_data_analyzer/bin/activate.  This will change the $PATH (add /path/to/ART_data_analyzer/bin) and $VIRTUAL_ENV = /path/to/ART_data_analyzer to let you use the virtual python environment. And type deactivate to end this virtual environment. Create a alias start_ART = “source /ART_data_analyzer/bin/activate” in .bash_profile or .profile to activate faster. More details check: https://virtualenv.pypa.io/en/stable/userguide/
https://packaging.python.org/tutorials/installing-packages/


How to install/use this python package:
This package is written in python scripting language to analyze the atomic configuration data generated by ART atomistic simulation software. There is no need to compile and build python code. This package can be put in any directory. Ensure to source the environment.sh before using this package. The purpose is to create the necessary environmental variables PYTHONPATH/PATH/TEST_PROJ for current bash sessions to find the python packages/exe scripts/test directory. The user need to change these environmental variables to point to their correct ART_data_analyzer package and the dir to ART data on their machine

How to use the package as a user work flow?
Though we say this is a data analysis package, the data here simply means ART raw data. To  obtain useful information from these atomistic simulation raw data, we need to perform the following steps:

After you set up the environment variables to get correct path to data and ART_data_analyzer.

First (data preprocessing), run event_filter.py to select non-redudant events saved in final_selected_events.json, or run your calculation, e.g.strain_calc.py if the user think that event redundancy check in event_filter.py would not filter many events.

Second(data calculation), if you run your calculation in first step, now you need to run event_filter to get correct statistical distribution. mp module will parallelize the calculation in a single test one by one

Third(data visualization and data analysis), run your data visualization (strain visualization) or data analysis module (correlation model) to extract the correct information, all these modules have been implemented with pathos.multiprocessing module through the util.operation_on_events function, where operation is a function to do data visualization or data analysis on a single event, pathos.mp will parallelize this operation function on single event one by one. This can be different from test level parallelization if test contains more than one final selected events. Correlation model module can be used to determine the average number of local involved atoms from initial to final configuration along with their histograms by implementing sklearn.linear_model.RANSACRegressor to find outliers, for the user to get reproducible result from RANSAC, user need to set up the seed integer in random_state option of RANSAC class and most importantly, choose the residual_threshold: maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values y.

Fourth, after the user get the average number of local atoms, then the user can involve the local atoms strain calculation model for all events by letting atom_list = “local”. This will redo the strain calculation for all local atoms defined as the number of atoms that are closest to the triggered atom which are defined in ART input file bash.sh. The strain_calculation.py module has implemented the finding of these local atoms automatically. All previous strain_results.pkl and displacement_results.pkl will be overwritten. Then the user can repeat the data visualization and data analysis (to develop correlation model) for locally involved atoms to avoid the local effect being averaged out by a large amount of atoms.


Executable files:

Currently, strain_calc.py is an exe python file, it performs displacement and atomic strain calculations and save all results and plots automatically in their corresponding locations. The calculation results have been rigorously verified with the results by Ovito.

event_filter.py ran rigorous redundancy check for each possible pair events which has passed the two stage 1 criteria, this script can ran before strain calculation to calculate minimal number of meaningful events or after strain calculation to help select the filtered events for the correct statistical distribution, 
whether to use this script depends on the percentage of events filtered by this criteria. Generally, the user is encouraged to use this script on a subset of tests such as 10 tests to check how many events are filtered. If negligible, It is possible to ignore this criteria.

strain_visualization.py generate the plots of all available calculated events that passed two stage 1 criteria, even when there is unfinished calculations or calculations are going on since it ignored uncalculated events, user can customize a subset of their interested tests to extract statistics

Before running this script, after user specify all the environmental variables in environmental.sh to match their own machine and source it. 

Nice features: 
support parallel calculation by multiprocessing module, the user can specify the number of processes in the input file

all calculations in the fly of strain calculations will be saved into a pkl file, such as nn_results.pkl, strain_results.pkl, and displacement_results.pkl for initial to saddle configuration, and saddle to final configuration for each event. When rerunning this calculation, these files will be directly read to prevent redundant running. It will skip the tests whose log.file or configuration files (e.g either min1000.dump or min1000) does not exist. It will also skip the test who do not have either accepted or selected events after calculation.

A re_calc argument is included with default False to use the existing calculations. However, when user want to try calculate with a new set of input parameters, they may need to invoke the re_calc to True.

All plots for visualizing the statistics of various physical quantities are automatically plotted and saved into their corresponding event locations that are specified by event_initial_str_sad_str_fin_str: for example, event_min1001_sad1002_min1002

These plots are automatically updated without redoing the calculation so that user can modify the plotting function to customize their own plot styles

The mean, std, max of displacement, shear strain, volumetric strain of each event are plotted into a histogram for all events, these plots are also automatically updated.

All events are accepted/selected based on their individual modules and have minimal interaction with the calculation modules. User can customize their own event selection criteria.

The user can specify the cut-off distance cut-off, the simulation box dimension box_dim and the total number of tests, num_of_procs in their data directory
{'cut_off':cut_off_distance,'box_dim':box_dim,'num_of_tests':num_of_tests,’num_of_procs’:num_of_procs}

If the num_of_procs == 1, it will run calculation in single process mode

If num_of_procs >1, it will invoke the multiprocessing module Pool class
num_of_procs can be found by nproc --all or grep -c processor /proc/cpuinfo in linux

In Mac, sysctl -n hw.ncpu to get logical CPUs

Crossplatform option here, default is to use all CPU cores by multiprocess.cpu_count() as implemented as default option here.

An example of cut_off_distance dict: {(1,1):3.7,(1,2):3.7,(2,2):3.7}
Means atom_type_1 and atom_type 1 cut_off_distance is 3.7, type 1 and type 2 is also 3.7 etc

Box_dim only support orthogonal currently, more to be implemented in the future

User need to input the number of tests in their data directory


This input for running the data calculation will be saved into os.environ['TEST_DIR']
as a pickle file for future reference if needed


In the future,
art_data: exe script
prompt the options that can use for various analyzing feature of the python package
Use sys.argparser to parse the user input from terminal
-calc/-c invoke the art_data_calculator exe 
-v invoke the art_data_visualization exe
-m invoke the correlation model exe by adopting the various data models implemented in scikit-learn machine learning package


 



